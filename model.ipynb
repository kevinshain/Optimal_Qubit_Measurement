{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "## Kevin Shain\n",
    "\n",
    "### 04/10/2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from numpy.random import random as rng\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import emcee\n",
    "import math\n",
    "from numpy import pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Generative Models](#Generative-Models)\n",
    "    * [Long time-scale errors](#Long-time-scale-errors)\n",
    "\t* [Model without drift or diffusion](#Model-without-drift-or-diffusion)\n",
    "\t* [Model with drift](#Model-with-drift)\n",
    "\t* [Model with drift and diffusion](#Model-with-drift-and-diffusion)\n",
    "* [Likelihood function](#Likelihood-function)\n",
    "* [Further ideas](#Further-ideas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because each measurement of a qubit probabilistically projects the a superposition state that exists somewhere on the qubit's Bloch sphere onto either the singlet or triplet state that is read out, my model is inherently probabilistic. Therefore, beyond adding random noise to the model, my model will generate probabilities of each measurement outcome and then randomly generate singlet(+1) or triplet(-1) for the measurement outcome.\n",
    "\n",
    "Another important aspect to these models is that there are different timescales for different effects. Some parameters, like measurement error, state preparation error, and non-orthogonal axis of rotation, are consistent throughout the lifetime of the qubit. This means that we can accurately measure values for those error parameters over thousands or millions of measurements. This means that for real-time or limited measurement purposes, we can focus on the parameters that vary over a shorter time scale, like $\\Delta B_z$ and its drift and diffusion. This tends to vary significantly on a time scale of a millisecond compared to measurements that take ~4 microseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long time-scale errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we can say that the state of our qubit without noise is,\n",
    "\n",
    "$$\\left|\\psi(t)\\right>=\\cos(\\pi\\Delta B_z t)\\left|+\\right>-i\\sin(\\pi\\Delta B_z t)\\left|-\\right>$$\n",
    "\n",
    "The probabilities for a given outcome as it depends on $\\Delta B_z$ is therefore,\n",
    "\n",
    "$$P(+|\\Delta B_z)=\\cos^2(\\pi\\Delta B_z t)$$\n",
    "$$P(-|\\Delta B_z)=\\sin^2(\\pi\\Delta B_z t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple model of the probabilities doesn't consider any errors, but it is important to note that it will still generate a random measurement outcome with defined proabilities so we will not be able to fit data from this model exactly even if we know $\\Delta B_z$. The first source of error that I'll consider is measurement error. Essentially, the readout of the qubit is done by measuring voltage across another quantum dot. This gives well differentiated Gaussian peaks of voltage depending on the qubit state. These peaks are then thresholded so that the singlet(triplet) peak is marked +(-). This thresholding process is imperfect as there is some small chance that a tail from the singlet Gaussian will leak into the triplet Gaussian or vice versa. Ideally, I would not do the thresholding as that is an extra step of data processing. It is unclear whether the added complexity of using raw voltage measurements is worth the computational cost, but I'll make a model that generates voltage measurements later so that comparisons can be made. For now, I'll simply work with $\\pm$ but include a measurement error term.\n",
    "\n",
    "With measurement error, the probability of measuring a + is essentially the probability of measuring + correctly plus the probability of measuring - incorrectly. If we say that the probability of incorrectly measuring +(-) is $\\eta_+(\\eta_-)$, this gives us,\n",
    "\n",
    "$$P(+|\\Delta B_z)=(1-\\eta_+)\\cos^2(\\pi\\Delta B_z t)+\\eta_-\\sin^2(\\pi\\Delta B_z t)$$\n",
    "$$P(-|\\Delta B_z)=\\eta_+\\cos^2(\\pi\\Delta B_z t)+(1-\\eta_-)\\sin^2(\\pi\\Delta B_z t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can account for state preparation error in a similar way. The initial state would ideally be purely singlet every time, but it actually is prepared in the triplet state some small percentage of the time. This means that the initial density matrix is,\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    " 1-\\epsilon & 0 \\\\ \n",
    " 0 & \\epsilon\n",
    " \\end{pmatrix}$$\n",
    " \n",
    "Accounting for just state preparation error, we would have,\n",
    "\n",
    "$$P(+|\\Delta B_z)=(1-\\epsilon)\\cos^2(\\pi\\Delta B_z t)+\\epsilon\\sin^2(\\pi\\Delta B_z t)$$\n",
    "$$P(-|\\Delta B_z)=\\epsilon\\cos^2(\\pi\\Delta B_z t)+(1-\\epsilon)\\sin^2(\\pi\\Delta B_z t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have axis of rotation error. This accounts for small differences in the electric potential between the two quantum dots that make up the qubit. This shifts the axis of rotation on the Bloch sphere about which the state vector rotates. This error alone leads to,\n",
    "\n",
    "$$P(+|\\Delta B_z)=\\cos^2(\\pi\\Delta B_z t)+\\delta\\sin^2(\\pi\\Delta B_z t)$$\n",
    "$$P(-|\\Delta B_z)=(1-\\delta)\\sin^2(\\pi\\Delta B_z t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the all of the errors together and using $\\cos^2(x)-\\sin^2(x)=\\cos(2x)$, we get,\n",
    "\n",
    "$$P(+|\\Delta B_z)=\\eta_-+\\frac{1}{2}(1-\\eta_+-\\eta_-)[1+(1-2\\epsilon)(\\delta+(1-\\delta)\\cos(2\\pi\\Delta B_zt))]$$ \n",
    "$$P(-|\\Delta B_z)=\\eta_++\\frac{1}{2}(1-\\eta_+-\\eta_-)[1-(1-2\\epsilon)(\\delta+(1-\\delta)\\cos(2\\pi\\Delta B_zt))]$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine these probabilities into one if we say the result of any measurement is $m_k=\\pm 1$ so,\n",
    "\n",
    "$$P(m_k|\\Delta B_z)=\\eta_-+\\frac{1}{2}(1-\\eta_+-\\eta_-)[1+m_k(1-\\epsilon)(\\delta+(1-\\delta)\\cos(2\\pi\\Delta B_zt))]$$\n",
    "\n",
    "We can reparameterize our long time-scale errors using\n",
    "\n",
    "$$\\alpha=\\eta_--\\eta_++(1-\\eta_--\\eta_+)(\\delta-2\\epsilon\\delta)$$\n",
    "$$\\beta=(1-\\eta_--\\eta_+)(1-\\delta)(1-2\\epsilon)$$\n",
    "\n",
    "to get,\n",
    "\n",
    "$$P(m_k|\\Delta B_z)=\\frac{1}{2}[1+m_k(\\alpha+\\beta\\cos(2\\pi\\Delta B_zt))]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though it seems that reparameterizing our errors sacrifices clarity, it turns out that $\\alpha$ and $\\beta$ are more easily extracted since repeatedly measuring immediately after preparation yields $p(+)=\\frac{1}{2}[1+\\alpha+\\beta]$ and simply making lots of measurements at random times and averaging gives $p(+)=\\frac{1}{2}[1+\\alpha]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model without drift or diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis lets us come to the simplest generative model which accounts for long time-scale errors, but doesn't account for drift and diffusion in $\\Delta B_z$. That will come later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measurement(Bz,t):\n",
    "    \"\"\"\n",
    "    returns either +1 or -1 with probability determined by the model for qubit evolution\n",
    "    and projective measurement\n",
    "    \n",
    "    Parameters:\n",
    "        Bz: Initial value of DeltaBz in the model (in MHz)\n",
    "        t: evolution time between state preparation and measurement (in nanoseconds)\n",
    "    \"\"\"\n",
    "    alpha=0.25\n",
    "    beta=0.67\n",
    "    BzHertz = Bz*10**6\n",
    "    tseconds = t*10**(-9)\n",
    "    pPlus = 1/2*(1+(alpha+beta*np.cos(2*pi*BzHertz*tseconds)))\n",
    "    x = np.random.rand(1)[0]\n",
    "    if (pPlus > x):\n",
    "        x = 1\n",
    "    else:\n",
    "        x = -1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, if we wanted to generate the voltage measurement, we could draw the voltage value from a Gaussian peaked at voltage corresponding to the singlet or triplet state. The mean and variance of these Gaussian peaks also have long time scales so we can specify them based on many previous measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measurementVoltage(Bz,t):\n",
    "    \"\"\"\n",
    "    returns a voltage value from a Gaussian peak based on the qubit state measurement.\n",
    "    The qubit state measurement is determined by the model for qubit evolution\n",
    "    and projective measurement.\n",
    "    \n",
    "    Parameters:\n",
    "        Bz: Initial value of DeltaBz in the model (in MHz)\n",
    "        t: evolution time between state preparation and measurement (in nanoseconds)\n",
    "    \"\"\"\n",
    "    alpha=0.25\n",
    "    beta=0.67\n",
    "    Smean = 4.5\n",
    "    Sstd = 0.3\n",
    "    Tmean = 5.5\n",
    "    Tstd = 0.3\n",
    "    \n",
    "    BzHertz = Bz*10**6\n",
    "    tseconds = t*10**(-9)\n",
    "    pPlus = 1/2*(1+(alpha+beta*np.cos(2*pi*BzHertz*tseconds)))\n",
    "    x = np.random.rand(1)[0]\n",
    "    if (pPlus > x):\n",
    "        voltage = np.random.norm(Smean,Sstd)\n",
    "    else:\n",
    "        voltage = np.random.norm(Tmean,Tstd)\n",
    "    return voltage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address drift in $\\Delta B_z$, we can use the approximation that $\\Delta B_z$ is constant within the measurement evolution time of a single measurement. A reasonable estimate of the drift is $25kHz/\\mu s$. The evolution times of a single measurement are 10-100 ns which justifies this approximation. However, each measurement in total takes $~4\\mu s$ so we have to take into account the drift between measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measurementDrift(Bz,t,drift,k):\n",
    "    \"\"\"\n",
    "    returns either +1 or -1 with probability determined by the model for qubit evolution\n",
    "    and projective measurement\n",
    "    \n",
    "    Parameters:\n",
    "        Bz: Initial value of DeltaBz in the model (in MHz)\n",
    "        t: evolution time between state preparation and measurement (in nanoseconds)\n",
    "        drift: rate of drift of DeltaBz in kHz/us\n",
    "        k: measurement index\n",
    "    \"\"\"\n",
    "    alpha=0.25\n",
    "    beta=0.67\n",
    "    driftHzs = drift*10**9 #units of Hz/s\n",
    "    timePerMeasurement = 4*10**(-6)\n",
    "    \n",
    "    BzHertz = Bz*10**6\n",
    "    tseconds = t*10**(-9)\n",
    "    \n",
    "    BzCurrent = BzHertz + driftHzs*timePerMeasurement*(k-1)\n",
    "    pPlus = 1/2*(1+(alpha+beta*np.cos(2*pi*BzCurrent*tseconds)))\n",
    "    x = np.random.rand(1)[0]\n",
    "    if (pPlus > x):\n",
    "        x = 1\n",
    "    else:\n",
    "        x = -1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with drift and diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address diffusion in $\\Delta B_z$, we again use the approximation that $\\Delta B_z$ is constant within the measurement evolution time of a single measurement. A reasonable estimate of the diffusion is $7 kHz^2/\\mu s$. We want to first define a function that executes a random walk of $\\Delta B_z$ that has the desired diffusion constant. This means, that we want,\n",
    "\n",
    "$$\\frac{\\partial P}{\\partial t}=D\\frac{\\partial^2 P}{\\partial x^2}$$\n",
    "\n",
    "We can solve this to say,\n",
    "\n",
    "$$P(x,t) = \\frac{1}{\\sqrt{4\\pi Dt}}e^{-x^2/4Dt}$$\n",
    "\n",
    "Therefore, if we say that each step in our random walk is $1\\mu s$, we can use our probability distribution to choose an updated $\\Delta B_z$ such that it follows the proper rate of diffusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomWalk(Bz):\n",
    "    \"\"\"\n",
    "    returns a vector with Bz values that have diffused over time. Each step is 1 microsecond\n",
    "    so in practice, each consequtive measurement pulls its DeltaBz from every fourth\n",
    "    value in the vector\n",
    "    \n",
    "    Parameters:\n",
    "        Bz: Initial value of DeltaBz in the model (in MHz)\n",
    "    \"\"\"\n",
    "    D = 7*10**(-6) # need to change from kHz^2 to MHz^2\n",
    "    \n",
    "    steps = 10000 # This allows for plenty of time to make ~100 measurements\n",
    "    BzDiffused = np.empty(steps)\n",
    "    BzDiffused[0] = Bz\n",
    "    sigma = np.sqrt(2*D)\n",
    "    for t in range(1,steps):\n",
    "        BzDiffused[t] = np.random.normal(BzDiffused[t-1], sigma)\n",
    "    return BzDiffused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measurementDriftDiffusion(BzDiffused,t,drift,k):\n",
    "    \"\"\"\n",
    "    returns either +1 or -1 with probability determined by the model for qubit evolution\n",
    "    and projective measurement\n",
    "    \n",
    "    Parameters:\n",
    "        BzDiffused: A vector of Bz values at 1 microsecond increments(in MHz)\n",
    "        t: evolution time between state preparation and measurement (in nanoseconds)\n",
    "        drift: rate of drift of DeltaBz in kHz/us\n",
    "        k: measurement index\n",
    "    \"\"\"\n",
    "    alpha=0.25\n",
    "    beta=0.67\n",
    "    driftHzs = drift*10**9 #units of Hz/s\n",
    "    timePerMeasurement = 4*10**(-6)\n",
    "    \n",
    "    \n",
    "    BzHertz = BzDiffused[4*k]*10**6 # each measurement looks at every 4th value of BzDiffused\n",
    "                                    # since a measurement takes 4 microseconds\n",
    "    tseconds = t*10**(-9)\n",
    "    \n",
    "    BzCurrent = BzHertz + driftHzs*timePerMeasurement*(k-1)\n",
    "    pPlus = 1/2*(1+(alpha+beta*np.cos(2*pi*BzCurrent*tseconds)))\n",
    "    x = np.random.rand(1)[0]\n",
    "    if (pPlus > x):\n",
    "        x = 1\n",
    "    else:\n",
    "        x = -1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some sense, obtaining a likelihood function from the model is straightforward because before projecting the state to + or -, our model gives probabilities of + or -. This means that to find the likelihood of a series of measurements, we can take the product of the probability that each measurement would result in the outcome that was actually obtained. This is complicated a bit by the fact that our parameters are changing between measurements in a non-deterministic way. Specifically, the diffusion means that we have uncertainty in our parameter $\\Delta B_z$ and that this uncertainty changes across the series of measurements. To account for this, I'll integrate my likelihood over the possible values of $\\Delta B_z$. Our parameters are thus the initial $\\Delta B_z$ and its drift, which together determine the mean value, and the diffusion constant that determines the variance of $\\Delta B_z$ values that we must integrate over to get the likelihood of each measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of a single measurement given a value of $\\Delta B_z$ is given by \n",
    "\n",
    "$$P(m_k|\\Delta B_z)=\\frac{1}{2}[1+m_k(\\alpha+\\beta\\cos(2\\pi\\Delta B_zt))]$$\n",
    "\n",
    "However, since we are saying that $\\Delta B_z$ drifts at some rate per measurement time, $v$, we should say,\n",
    "\n",
    "$$P(m_k|\\Delta B_z)=\\frac{1}{2}[1+m_k(\\alpha+\\beta\\cos(2\\pi(\\Delta B_z+vk)t)]$$\n",
    "\n",
    "Finally, the diffusion of $\\Delta B_z$ with a diffusion constant per measurement time, $D$, means that we can say there is a distribution for $\\Delta B_z$ at each measurement given by,\n",
    "\n",
    "$$\\Delta B_{z_k}=\\frac{1}{\\sqrt{4\\pi Dk}}e^{-[\\Delta B_{z_k}-(\\Delta B_{z_0}+vk)]^2/4Dk}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining everything we have for a single measurement, we have,\n",
    "\n",
    "$$P(m_k|\\Delta B_{z_0},v,D)=\\int_{-\\infty}^\\infty\\frac{1}{\\sqrt{4\\pi Dk}}e^{-[\\Delta B_{z_k}-(\\Delta B_{z_0}+vk)]^2/4Dk}\\frac{1}{2}[1+m_k(\\alpha+\\beta\\cos(2\\pi(\\Delta B_z+vk)t)]d\\Delta B_{z_k}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can write the likelihood for a series of measurements as,\n",
    "\n",
    "$$P(\\{m\\}|\\Delta B_{z_0},v,D)=\\prod_k\\int_{-\\infty}^\\infty\\frac{1}{\\sqrt{4\\pi Dk}}e^{-[\\Delta B_{z_k}-(\\Delta B_{z_0}+vk)]^2/4Dk}\\frac{1}{2}[1+m_k(\\alpha+\\beta\\cos(2\\pi(\\Delta B_z+vk)t)]d\\Delta B_{z_k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe that this likelihood is computable. I don't see a reason why it shouldn't be. However, I have a notion that I can use the fact that the likelihood of each measurement is in the form of a cosine to turn the likelihood or the posterior into a finite Fourier series. I can do this using the identity that transforms a product of cosines into a sum of cosines. My thought is that this will significantly save on computation time because certain expectation values can be calculated just from Fourier coefficients without having to evaluate the likelihood at various values of $\\Delta B_z$. I've been reading a book on spectral analysis and I believe that this should be a viable technique. Furthermore, drift and diffusion can be handled very naturally in the frequency representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
